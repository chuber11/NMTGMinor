#include <vector>
#include <math.h>
#include <iostream>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <cuda_profiler_api.h>
#include <fstream>
#include <exception>
#include <memory>

#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>
#include <torch/extension.h>

#include "softmax_apex.h"
#include "gemm_blaslt.cuh"
#include "strided_batched_gemm_blaslt.cuh"

using namespace torch::indexing;

// symbol to be automatically resolved by PyTorch libs

namespace multihead_attn {
namespace relative_partially_learnable_self {
namespace cublas_gemmex {

std::vector<torch::Tensor> fwd_cuda(
                               bool                 is_training,
                               bool                 use_time_mask,
                               int                  heads,
                               torch::Tensor const& inputs,
                               torch::Tensor const& pos,
                               torch::Tensor const& input_weights,
                               torch::Tensor const& output_weights,
                               torch::Tensor const& pos_weights,
                               torch::Tensor const& input_biases,
                               torch::Tensor const& output_biases,
                               torch::Tensor const& pos_biases,
                               torch::Tensor const& r_w_bias,
                               torch::Tensor const& r_r_bias,
                               torch::Tensor const& pad_mask,
                               float                dropout_prob,
                               torch::Tensor lt_workspace
                                                  )
{
    const int   embed_dim         = inputs.size(2);
    const int   sequences         = inputs.size(1);
    const int   q_seq_len         = inputs.size(0);
    const int   k_seq_len         = q_seq_len;
    const int   r_seq_len         = pos.size(0);
    const int   batches           = sequences * q_seq_len;
    const int   head_dim          = embed_dim / heads;
    const int   output_lin_dim    = 3 * embed_dim;
    const int   attn_batches      = heads * sequences;
    const int   lead_dim_qkv      = attn_batches * 3 * head_dim;
    const int   lead_dim          = attn_batches * head_dim;
    const int   batch_stride_qkv  = 3 * head_dim;
    const int   batch_stride      = head_dim;
    const int   dropout_elems     = attn_batches * q_seq_len * k_seq_len;
    const float alpha             = 1.0;
    const float beta_zero         = 0.0;
//     const float beta_one          = 1.0;
    const float scale             = 1.0 / sqrt(static_cast<float>(head_dim));

    // There is no reason to use more than one stream as every kernel is
    // sequentially dependent
    cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
    cudaStream_t   stream = at::cuda::getCurrentCUDAStream().stream();
    cublasSetStream(handle, stream);

    // 3 Intermediate Results + Output (Note: dropout intermediates are generated by ATen library code)
    auto act_options  = inputs.options().requires_grad(false);
    auto mask_options = act_options.dtype(torch::kUInt8);

    torch::Tensor rr_head_q = torch::empty({q_seq_len, attn_batches, head_dim}, act_options);
    torch::Tensor rw_head_q = torch::empty({q_seq_len, attn_batches, head_dim}, act_options);
    torch::Tensor null_tensor = torch::empty({0}, act_options);
    torch::Tensor input_lin_results = torch::empty({q_seq_len, sequences, output_lin_dim}, act_options);
    torch::Tensor pos_lin_results   = torch::empty({r_seq_len, sequences, embed_dim}, act_options);
    torch::Tensor attn_scores_ac    = torch::empty({attn_batches, q_seq_len, k_seq_len},   act_options);
    torch::Tensor attn_scores_bd    = torch::empty({attn_batches, q_seq_len, r_seq_len},   act_options);
    torch::Tensor dropout_results   = torch::empty({attn_batches, q_seq_len, k_seq_len},   act_options);
    torch::Tensor dropout_mask      = torch::empty({attn_batches, q_seq_len, k_seq_len},   mask_options);
    torch::Tensor matmul2_results   = torch::empty({q_seq_len, attn_batches, head_dim},    act_options);
    torch::Tensor outputs           = torch::empty_like(inputs, act_options);

    // Input Linear Results Pointers to Q, K, and V of interviewed activations
    void* q_lin_results_ptr   = static_cast<void*>(input_lin_results.data_ptr());
    void* k_lin_results_ptr   = static_cast<void*>(static_cast<half*>(input_lin_results.data_ptr()) + head_dim);
    void* v_lin_results_ptr   = static_cast<void*>(static_cast<half*>(input_lin_results.data_ptr()) + 2*head_dim);
    void* pos_lin_results_ptr = static_cast<void*>(pos_lin_results.data_ptr());

    torch::Tensor query = input_lin_results.view({q_seq_len, sequences*heads,
                                                  3, head_dim}).index({Slice(), Slice(), 0, Slice()});


    void* rw_head_q_ptr   = static_cast<void*>(rw_head_q.data_ptr());
    void* rr_head_q_ptr   = static_cast<void*>(rr_head_q.data_ptr());

    // Softmax Intermediate Result Ptr (used by Matmul1 -> Softmax)
    void* attn_scores_ac_ptr = static_cast<void*>(attn_scores_ac.data_ptr());
    void* attn_scores_bd_ptr = static_cast<void*>(attn_scores_bd.data_ptr());
    void* dropout_results_ptr = static_cast<void*>(dropout_results.data_ptr());

    void* lt_workspace_ptr = static_cast<void*>(lt_workspace.data_ptr());

    TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));

    // Input Linear Fwd
    int cublas_status = 1;

    // Linear projection for positions
    cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            embed_dim,  // m
            sequences * r_seq_len,    // n
            embed_dim,  // k
            &alpha, /* host pointer */
            static_cast<const void*>(pos_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(pos.data_ptr()),
            embed_dim,
            &beta_zero, /* host pointer */
            pos_lin_results_ptr,
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<const void*>(pos_biases.data_ptr()));

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM pos forward failed with %d\n", cublas_status);
      exit(0);
    }

    // Linear projection for inputs QKV
    cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            output_lin_dim,
            batches,
            embed_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(input_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(inputs.data_ptr()),
            embed_dim,
            &beta_zero, /* host pointer */
            q_lin_results_ptr,
            output_lin_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<const void*>(input_biases.data_ptr()));

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM QKV forward failed with %d\n", cublas_status);
      exit(0);
    }

    // Just view and then copy (with broadcast)
    rr_head_q.view({q_seq_len * sequences, heads, head_dim}).copy_(r_r_bias.unsqueeze(0));
    rw_head_q.view({q_seq_len * sequences, heads, head_dim}).copy_(r_w_bias.unsqueeze(0));

    // both of those tensors have the same size with query now
    rw_head_q.add_(query);
    rr_head_q.add_(query);

    //    matmul_ac batched GEMMs
    //    rw_head_q: [len_q, bsz*heads, head_dim]
    //    keys:      [len_k, bsz*heads, head_dim]
    //    per_batch: [len_q x head_dim] \mul [head_dim, len_k]
    //    cublas-cm  [len_k, head_dim]  \mul [head_dim, len_q]
    //    result-cm  [len_k x len_q x bsz*heads]
    //    MatMul1 of Dot-Product Attention Plus scaling by 1/Sqrt(head size)

    cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            k_seq_len,
            q_seq_len,
            head_dim,
            &scale, /* host pointer */
            static_cast<const void*>(k_lin_results_ptr),
            lead_dim_qkv,
            batch_stride_qkv,
            static_cast<const void*>(rw_head_q_ptr),
            lead_dim,
            batch_stride,
            &beta_zero, /* host pointer */
            static_cast<void*>(attn_scores_ac_ptr), // C
            k_seq_len, // ldc
            k_seq_len*q_seq_len, // stride c
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("strided batched gemm MatmulAC forward failed with %d\n", cublas_status);
      exit(0);
    }

    //    matmul_bd batched GEMMs
    //    rr_head_q: [len_q, bsz*heads, head_dim]
    //    pos:      [len_r, bsz*heads, head_dim]
    //    per_batch: [len_q x head_dim] \mul [head_dim, len_r]
    //    cublas-cm  [len_r, head_dim]  \mul [head_dim, len_q]
    //    result-cm  [len_k x len_q x bsz*heads]
    //    MatMul2 of Dot-Product Attention Plus scaling by 1/Sqrt(head size)

    cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            r_seq_len,
            q_seq_len,
            head_dim,
            &scale, /* host pointer */
            static_cast<const void*>(pos_lin_results_ptr),
            lead_dim,  // heads * sequence * head_dim
            batch_stride,  // = head_dim
            static_cast<const void*>(rr_head_q_ptr),
            lead_dim,
            batch_stride,
            &beta_zero, /* host pointer */
            static_cast<void*>(attn_scores_bd_ptr), // C
            r_seq_len, // ldc
            r_seq_len*q_seq_len, // stride c
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("strided batched gemm MatmulBD forward failed with %d\n", cublas_status);
      exit(0);
    }

    //    Relative Shift
    //     zero_pad = torch.zeros((bsz, x.size(1), 1),
    //                                    device=x.device, dtype=x.dtype)
    //
    //     # padded into [T x T+1 x (B x H)]
    //     x_padded = torch.cat([zero_pad, x], dim=2)
    //
    //     # view into [T+1 x T x (BxH)]
    //     x_view = x_padded.view(bsz, x.size(2) + 1, x.size(1))
    //
    //     # remove the first collumn
    //     x = x_view[:, 1:].view_as(x)

    // Relative Shift
    torch::Tensor zero_pad = at::zeros({attn_batches, q_seq_len, 1}, act_options);
    torch::Tensor padded_tensor = at::cat({zero_pad, attn_scores_bd}, 2);
    torch::Tensor view_tensor = padded_tensor.view({attn_batches, r_seq_len+1, q_seq_len});
    attn_scores_bd = view_tensor.index({Slice(), Slice(1, None), Slice()}).view({attn_batches, q_seq_len, r_seq_len});

    // Cut off the extra length
    attn_scores_bd = attn_scores_bd.index({Slice(), Slice(), Slice(None, k_seq_len)});
    attn_scores_ac.add_(attn_scores_bd);

    if (use_time_mask){
        attn_scores_ac.masked_fill_(pad_mask, -std::numeric_limits<float>::infinity());
    } else {
        attn_scores_ac.view({sequences, heads, q_seq_len, k_seq_len}).masked_fill_(pad_mask,
                                                                              -std::numeric_limits<float>::infinity());
    }
    // Padded Softmax
    bool softmax_success = false;

    if (is_training && dropout_prob > 0.0f) {
      // This function fuses softmax-dropout-pad (and dropout inplace)
      softmax_success = dispatch_softmax_dropout<half, half, float>(
                           reinterpret_cast<half*>(dropout_results_ptr),
                           (is_training) ? reinterpret_cast<uint8_t*>(dropout_mask.data_ptr<uint8_t>()) : nullptr,
                           reinterpret_cast<const half*>(attn_scores_ac_ptr),
      		               dropout_elems,
                           k_seq_len,
                           k_seq_len,
                           attn_batches*q_seq_len,
      		               1.0f-dropout_prob,
		                   stream);
    } else {
      softmax_success = dispatch_softmax<half, half, float>(
                             reinterpret_cast<half*>(dropout_results_ptr), // this is actually softmax results, but making it consistent for the next function
                             reinterpret_cast<const half*>(attn_scores_ac_ptr),
                             k_seq_len,
                             k_seq_len,
                             attn_batches*q_seq_len,
                             stream);  // pad batch strides

      // if dropout = 0: remove this tensor because its not necessary in the backward pass
      attn_scores_ac = null_tensor;
    }

    assert(softmax_success);

    cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            head_dim,
            q_seq_len,
            k_seq_len,
            &alpha, /* host pointer */
            static_cast<const void*>(v_lin_results_ptr),  // A:
            lead_dim_qkv,
            batch_stride_qkv,
            static_cast<const void*>(dropout_results.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta_zero, /* host pointer */
            static_cast<void*>(matmul2_results.data_ptr()), // C
            head_dim*attn_batches,
            head_dim,
            attn_batches,
            lt_workspace_ptr,
            1 << 22,
            stream);

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM Context forward failed with %d\n", cublas_status);
      exit(0);
    }

    cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            embed_dim,
            batches,
            embed_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(output_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(matmul2_results.data_ptr()),
            embed_dim,
            &beta_zero, /* host pointer */
            static_cast<void*>(outputs.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<const void*>(output_biases.data_ptr()));

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
        printf("GEMM output forward failed with %d\n", cublas_status);
        exit(0);
    }
    TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));

    return {
        input_lin_results, pos_lin_results,
//         rr_head_q, rw_head_q,
        attn_scores_ac, dropout_results, dropout_mask,
        matmul2_results,
        outputs
    };

}

std::vector<torch::Tensor> bwd_cuda(
                               int                  heads,
                               torch::Tensor const& output_grads,
                               torch::Tensor const& matmul2_results,
                               torch::Tensor const& dropout_results,
                               torch::Tensor const& attn_scores,
                               torch::Tensor const& input_lin_results,
                               torch::Tensor const& pos_lin_results,
//                                torch::Tensor const& rw_head_q,
//                                torch::Tensor const& rr_head_q,
                               torch::Tensor const& r_w_bias,
                               torch::Tensor const& r_r_bias,
                               torch::Tensor const& inputs,
                               torch::Tensor const& pos,
                               torch::Tensor const& input_weights,
                               torch::Tensor const& output_weights,
                               torch::Tensor const& pos_weights,
                               torch::Tensor const& dropout_mask,
                               float                dropout_prob,
                               torch::Tensor lt_workspace
                                                  )
{
    const int   embed_dim         = inputs.size(2);
    const int   sequences         = inputs.size(1);
    const int   q_seq_len         = inputs.size(0);
    const int   r_seq_len         = pos.size(0);
    const int   k_seq_len         = q_seq_len;
    const int   batches         = sequences * q_seq_len;
    const int   head_dim          = embed_dim / heads;
    const int   output_lin_dim  = 3 * embed_dim;
    const int   attn_batches      = heads * sequences;
    const int   lead_dim_qkv        = attn_batches * 3 * head_dim;
    const int   batch_stride_qkv   = 3 * head_dim;
    const int   lead_dim        = attn_batches * head_dim;
    const int   batch_stride   = head_dim;
    //  const int   dropout_elems     = attn_batches * q_seq_len * k_seq_len;
    const float alpha             = 1.0;
    const float beta              = 0.0;
    const float scale             = 1.0 / sqrt(static_cast<float>(head_dim));
    auto act_options  = inputs.options().requires_grad(false);

    // TODO: Streams can be used in Backprop but I haven't added more than one
    // in my first attempt to create the code
    cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
    cudaStream_t   stream = at::cuda::getCurrentCUDAStream().stream();
    cublasSetStream(handle, stream);

    // Output Tensor Allocations
    torch::Tensor input_grads         = torch::empty_like(inputs);
//     torch::Tensor pos_grads           = torch::empty_like(pos);
    torch::Tensor input_weights_grads  = torch::empty_like(input_weights);
    torch::Tensor output_weights_grads = torch::empty_like(output_weights);
    torch::Tensor pos_weights_grads = torch::empty_like(pos_weights);
    // Intermediate Tensor Allocations
    at::Tensor output_lin_grads       = torch::empty_like(matmul2_results);
    at::Tensor matmul2_grads          = torch::empty_like(dropout_results);
    at::Tensor input_lin_output_grads = torch::empty_like(input_lin_results);
    at::Tensor pos_lin_output_grads = torch::empty_like(pos_lin_results);

    torch::Tensor rr_head_q = torch::empty({q_seq_len, attn_batches, head_dim}, act_options);
    torch::Tensor rw_head_q = torch::empty({q_seq_len, attn_batches, head_dim}, act_options);
    void* rw_head_q_ptr   = static_cast<void*>(rw_head_q.data_ptr());
    void* rr_head_q_ptr   = static_cast<void*>(rw_head_q.data_ptr());

    auto q_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr());
    auto k_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr()) + head_dim;
    auto v_lin_results_ptr = static_cast<half*>(input_lin_results.data_ptr()) + 2*head_dim;
    void* pos_lin_results_ptr = static_cast<void*>(pos_lin_results.data_ptr());

    torch::Tensor query = input_lin_results.view({q_seq_len, sequences*heads,
                                                  3, head_dim}).index({Slice(), Slice(), 0, Slice()});

    auto q_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr());

    auto k_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr()) + head_dim;
    auto v_lin_grads_ptr = static_cast<half*>(input_lin_output_grads.data_ptr()) + 2*head_dim;
    auto pos_lin_grads_ptr = static_cast<half*>(pos_lin_output_grads.data_ptr());

    at::Tensor input_biases_grads = torch::empty({output_lin_dim}, inputs.type());
    at::Tensor output_biases_grads = torch::empty({embed_dim}, inputs.type());
    at::Tensor pos_biases_grads = torch::empty({embed_dim}, inputs.type());

    void* lt_workspace_ptr = static_cast<void*>(lt_workspace.data_ptr());

    // need a tensor at this position
    torch::Tensor queries_grads = input_lin_output_grads.view({q_seq_len, sequences*heads,
                                                  3, head_dim}).index({Slice(), Slice(), 0, Slice()});

    torch::Tensor queries_grads_bd = torch::empty({q_seq_len, sequences*heads, head_dim}, inputs.type());
    // torch::empty_like(queries_grads);

    TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));

    // Recompute these two tensors (just summation)
    // Just view and then copy (with broadcast)
    rr_head_q.view({q_seq_len * sequences, heads, head_dim}).copy_(r_r_bias.unsqueeze(0));
    rw_head_q.view({q_seq_len * sequences, heads, head_dim}).copy_(r_w_bias.unsqueeze(0));

    // both of those tensors have the same size with query now
    rw_head_q.add_(query);
    rr_head_q.add_(query);
    int cublas_status = 1;

    // Output Linear Dgrad
    cublas_status = gemm_bias_lt(
                (cublasLtHandle_t)handle,
                CUBLAS_OP_N,
                CUBLAS_OP_N,
                embed_dim,
                batches,
                embed_dim,
                &alpha, /* host pointer */
                static_cast<const void*>(output_weights.data_ptr()),
                embed_dim,
                static_cast<const void*>(output_grads.data_ptr()),
                embed_dim,
                &beta, /* host pointer */
                static_cast<void*>(output_lin_grads.data_ptr()),
                embed_dim,
                lt_workspace_ptr, // TODO: get lt_workspace
                1 << 22,
                stream,
                false,
                static_cast<const void*>(nullptr));

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output lin grad backward failed with %d\n", cublas_status);
      exit(0);
    }

    cublas_status = gemm_bgradb_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            embed_dim,
            embed_dim,
            batches,
            &alpha, /* host pointer */
            static_cast<const void*>(matmul2_results.data_ptr()),
            embed_dim,
            static_cast<const void*>(output_grads.data_ptr()),
            embed_dim,
            &beta, /* host pointer */
            static_cast<void*>(output_weights_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr,
            1 << 22,
            stream,
            true,
            static_cast<void*>(output_biases_grads.data_ptr()));

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("GEMM output backward failed with %d\n", cublas_status);
      exit(0);
    }

    // MatMul2 Dgrad1
    cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            k_seq_len,
            q_seq_len,
            head_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(v_lin_results_ptr),  // A:
            lead_dim_qkv,
            batch_stride_qkv,
            static_cast<const void*>(output_lin_grads.data_ptr()),
            head_dim*attn_batches,
            head_dim,
            &beta, /* host pointer */
            static_cast<void*>(matmul2_grads.data_ptr()), // C
            k_seq_len,
            k_seq_len*q_seq_len,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM backward 1 failed with %d\n", cublas_status);
      exit(0);
    }

    // Matmul2 Dgrad2
    cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            head_dim,
            k_seq_len,
            q_seq_len,
            &alpha, /* host pointer */
            static_cast<const void*>(output_lin_grads.data_ptr()),  // A:
            head_dim*attn_batches,
            head_dim,
            static_cast<const void*>(dropout_results.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(v_lin_grads_ptr), // C
            lead_dim_qkv,
            batch_stride_qkv,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
      printf("Strided Batched GEMM backward 2 failed with %d\n", cublas_status);
      exit(0);
    }

    // bool softmax_success = false;

    if ( dropout_prob > 0.0f) {
      dispatch_softmax_dropout_backward_recompute<half, half, float, false>(
                                     static_cast<half*>(matmul2_grads.data_ptr()),
                                     static_cast<const half*>(matmul2_grads.data_ptr()),
                                     reinterpret_cast<const half*>(attn_scores.data_ptr()), // need this to recompute softmax
                                     //reinterpret_cast<half const*>(pad_mask.data_ptr()),
                                     static_cast<uint8_t const*>(dropout_mask.data_ptr()),
                                     1.0/(1.0-dropout_prob),
                                     k_seq_len,
                                     k_seq_len,
                                     attn_batches*q_seq_len,
                                     stream);
    //       if dropout == 0 then we don't need to recompute (because dropout_results == softmax_results)
    } else {
      dispatch_softmax_backward_norecompute<half, half, float, false>(
                                 static_cast<half*>(matmul2_grads.data_ptr()),
                                 static_cast<const half*>(matmul2_grads.data_ptr()),
                                 reinterpret_cast<const half*>(dropout_results.data_ptr()),
                                 k_seq_len,
                                 k_seq_len,
                                 attn_batches*q_seq_len,
                                 stream);
    }

    // after softmax we have attn_score_grads = matmul2_grads

    // Matmul1 Dgrad1: first grads to the query_grad: multiply grads with keys -> queries_grad_ac
//     torch.baddbmm(queries_grads.transpose(0, 1), matmul_ac_grads, keys.transpose(0, 1),
//                       out=queries_grads.transpose(0, 1), beta=0.0, alpha=scale_t[0])
    cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            head_dim,
            q_seq_len,
            k_seq_len,
            &scale, /* host pointer */
            static_cast<const void*>(k_lin_results_ptr),  // A:
            lead_dim_qkv,
            batch_stride_qkv,
            static_cast<const void*>(matmul2_grads.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(q_lin_grads_ptr), // C
            lead_dim_qkv,
            batch_stride_qkv,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
        printf("GEMM backward 1 failed with %d\n", cublas_status);
        exit(0);
    }

    auto r_w_bias_grads = queries_grads.view({q_seq_len * sequences, heads, head_dim}).sum(0, false);

    // have to add the zeroed gradients
//     torch::empty({attn_batches, q_seq_len, k_seq_len},   act_options);
    torch::Tensor matmul_bd_grads;
    if (r_seq_len > k_seq_len)  {
//         grad_cut = matmul_bd_grads.new_zeros((matmul_bd_grads.size(0), matmul_bd_grads.size(1), len_r - len_q))
//                 matmul_bd_grads = torch.cat([matmul_bd_grads, grad_cut], dim=-1)
        torch::Tensor grad_cut = torch::zeros({attn_batches, q_seq_len, (r_seq_len - k_seq_len)}, act_options);
        matmul_bd_grads = at::cat({matmul2_grads, grad_cut}, 2);

    } else {
        matmul_bd_grads = matmul2_grads;
    }

    // have to reverse the shift op
//     grad_x_view = grad_x.view(bsz, len_r, len_q)
//
//     zero_pad = torch.zeros((bsz, 1, len_q), device=grad_x.device, dtype=grad_x.dtype)
//
//     # grad_x should have size B x len_q x len_r
//     # x_view should have size B x len_q+1 x len_r
//
//     # put the zeros into the missing gradients
//     grad_x_view = torch.cat([zero_pad, grad_x_view], dim=1)
//     grad_x_padded = grad_x_view.view(bsz, len_q, len_r + 1)
//
//     # because the first index in the padded dim was from zero_pad
//     grad_output = grad_x_padded[:, :, 1:]
    // Reverse shift CORRECT
    torch::Tensor grad_x_view = matmul_bd_grads.view({attn_batches, r_seq_len, q_seq_len});
    torch::Tensor zero_pad = torch::zeros({attn_batches, 1, q_seq_len}, act_options);
    torch::Tensor grad_x_cat = at::cat({zero_pad, grad_x_view}, 1);
    torch::Tensor grad_x_padded = grad_x_cat.view({attn_batches, q_seq_len, r_seq_len + 1});
    matmul_bd_grads = grad_x_padded.index({Slice(), Slice(), Slice(1, None)});

//     cublas_status = strided_batched_gemm_lt(
//             (cublasLtHandle_t)handle,
//             CUBLAS_OP_N,
//             CUBLAS_OP_N,
//             head_dim,
//             q_seq_len,
//             r_seq_len,
//             &scale, /* host pointer */
//             static_cast<const void*>(pos_lin_results_ptr),  // A: [lenr * attn_batches * head_dim]
//             lead_dim,  // attn_batches * head_dim;
//             batch_stride, // head_dim
//             static_cast<const void*>(matmul_bd_grads.data_ptr()), // [attnbatches * lenq * lenr]
//             r_seq_len,
//             r_seq_len*q_seq_len,
//             &beta, /* host pointer */
//             static_cast<void*>(queries_grads_bd.data_ptr()), // [len_q * attn_batches * head_dim]
//             lead_dim,
//             batch_stride,
//             attn_batches,  // batch = heads * bsz
//             lt_workspace_ptr,
//             1 << 22,
//             stream);
//
//     if (cublas_status != CUBLAS_STATUS_SUCCESS) {
//         printf("GEMM backward bd failed with %d\n", cublas_status);
//         exit(0);
//     }

//     torch.baddbmm(queries_grads_bd.transpose(0, 1), matmul_bd_grads, r_head_k.transpose(0, 1),
//                           out=queries_grads_bd.transpose(0, 1), beta=0.0, alpha=scale_t[0]);
//
    torch::Tensor r_head_k = pos_lin_results.view({r_seq_len, sequences * heads, head_dim});
    queries_grads_bd.transpose(0, 1).baddbmm_(matmul_bd_grads, r_head_k.transpose(0, 1), 0.0, scale);

    auto r_r_bias_grads = queries_grads_bd.view({q_seq_len * sequences, heads, head_dim}).sum(0, false);

    queries_grads.add_(queries_grads_bd);

    // Matmul1 Dgrad2
//     gemm_switch_fp32accum(     state,
//                              a_layout_n,
//                              b_layout_t,
//                              head_dim,
//                              k_seq_len,
//                              q_seq_len,
//                              scale,
//                              static_cast<half*>(rw_head_q_ptr),
//                              lead_dim,  // because rw_head_q is not a sub-mat
//                              batch_stride,
//                              static_cast<half*>(matmul2_grads.data_ptr()),
//                              k_seq_len,
//                              k_seq_len*q_seq_len,
//                              beta,
//                              k_lin_grads_ptr,
//                              lead_dim_qkv,
//                              batch_stride_qkv,
//                              attn_batches);
    // strided blaslt matmul  matmul2_grads * rw_head_q ->  k_lin_grads_ptr
    cublas_status = strided_batched_gemm_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            head_dim,
            k_seq_len,
            q_seq_len,
            &scale, /* host pointer */
            static_cast<const void*>(rw_head_q_ptr),  // A:
            lead_dim,
            batch_stride,
            static_cast<const void*>(matmul2_grads.data_ptr()),
            k_seq_len,
            k_seq_len*q_seq_len,
            &beta, /* host pointer */
            static_cast<void*>(k_lin_grads_ptr), // C
            lead_dim_qkv,
            batch_stride_qkv,
            attn_batches,  // batch = heads * bsz
            lt_workspace_ptr,
            1 << 22,
            stream);

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
        printf("GEMM backward 2 failed with %d\n", cublas_status);
        exit(0);
    }

    // strided blaslt matmul  matmul2_grads * rr_head_q ->  pos_lin_grads_ptr
//     cublas_status = strided_batched_gemm_lt(
//             (cublasLtHandle_t)handle,
//             CUBLAS_OP_N,
//             CUBLAS_OP_T,
//             head_dim,
//             r_seq_len,
//             q_seq_len,
//             &scale, /* host pointer */
//             static_cast<const void*>(rr_head_q_ptr),  // A:
//             lead_dim,
//             batch_stride,
//             static_cast<const void*>(matmul_bd_grads.data_ptr()),
//             r_seq_len,
//             r_seq_len*q_seq_len,
//             &beta, /* host pointer */
//             static_cast<void*>(pos_lin_grads_ptr), // r_head_k_grad
//             lead_dim,
//             batch_stride,
//             attn_batches,  // batch = heads * bsz
//             lt_workspace_ptr,
//             1 << 22,
//             stream);
//
//     if (cublas_status != CUBLAS_STATUS_SUCCESS) {
//         printf("GEMM backward rr_head_q x matmul_bd_grads failed with %d\n", cublas_status);
//         exit(0);
//     }

    at::Tensor r_head_k_grad = pos_lin_output_grads.view({r_seq_len, sequences * heads, head_dim});
    r_head_k_grad.transpose(0, 1).baddbmm_(matmul_bd_grads.transpose(1, 2), rr_head_q.transpose(0, 1), 0.0, scale);
    pos_lin_output_grads = r_head_k_grad.view({r_seq_len, sequences, heads*head_dim});

    // Input Linear Dgrad
    cublas_status = gemm_bias_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            embed_dim,
            batches,
            output_lin_dim,
            &alpha, /* host pointer */
            static_cast<const void*>(input_weights.data_ptr()),
            embed_dim,
            static_cast<const void*>(input_lin_output_grads.data_ptr()),
            output_lin_dim,
            &beta, /* host pointer */
            static_cast<void*>(input_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            false,
            static_cast<const void*>(nullptr));

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
        printf("GEMM output backward final input failed with %d\n", cublas_status);
        exit(0);
    }

    // Input Linear Wgrad
    cublas_status = gemm_bgradb_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            embed_dim,
            output_lin_dim,
            batches,
            &alpha, /* host pointer */
            static_cast<const void*>(inputs.data_ptr()),
            embed_dim,
            reinterpret_cast<const void*>(q_lin_grads_ptr),
            output_lin_dim,
            &beta, /* host pointer */
            static_cast<void*>(input_weights_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<void*>(input_biases_grads.data_ptr()));

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
        printf("GEMM input weight backward failed with %d\n", cublas_status);
        exit(0);
    }

    // Input Linear Wgrad
    cublas_status = gemm_bgradb_lt(
            (cublasLtHandle_t)handle,
            CUBLAS_OP_N,
            CUBLAS_OP_T,
            embed_dim,
            embed_dim,
            sequences * r_seq_len,
            &alpha, /* host pointer */
            static_cast<const void*>(pos.data_ptr()),
            embed_dim,
            reinterpret_cast<const void*>(pos_lin_grads_ptr),
            embed_dim,
            &beta, /* host pointer */
            static_cast<void*>(pos_weights_grads.data_ptr()),
            embed_dim,
            lt_workspace_ptr, // TODO: get lt_workspace
            1 << 22,
            stream,
            true,
            static_cast<void*>(pos_biases_grads.data_ptr()));

    if (cublas_status != CUBLAS_STATUS_SUCCESS) {
        printf("GEMM pos weight backward failed with %d\n", cublas_status);
        exit(0);
    }

    TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));

    return  {
        input_grads,
        input_weights_grads,
        pos_weights_grads,
        output_weights_grads,
        input_biases_grads,
        pos_biases_grads,
        output_biases_grads,
        r_w_bias_grads, r_r_bias_grads
    };

}


}
}
}